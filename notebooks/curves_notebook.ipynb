{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # header\n",
    "import sys\n",
    "sys.path.append(r\"../\")\n",
    "from src.curves_utils import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_folder = r\"../pretrained/curves\"\n",
    "results_folder, logger = startup_folders(start_folder, name=f\"exp_curves\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = load_dicts(start_folder, \"model_params\")\n",
    "tasks = load_dicts(start_folder, \"tasks\")\n",
    "train_params = load_dicts(start_folder, \"train_params\")\n",
    "DeVice, num_workers, pin_memory = get_device()\n",
    "logger.info(f\"model_params: {model_params}\")\n",
    "logger.info(f\"tasks: {tasks}\")\n",
    "logger.info(f\"train_params: {train_params}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # setting up the tasks\n",
    "tasks[\"CurveTracing\"][\"composer\"] = CurveTracing\n",
    "tasks[\"CurveTracing\"][\"datasets\"] = []\n",
    "tasks[\"CurveTracing\"][\"dataloaders\"] = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datasets and dataloaders\n",
    "DeVice, num_workers, pin_memory = get_device()\n",
    "for o in tasks:\n",
    "    tasks[o][\"datasets\"].append(tasks[o][\"composer\"](n_samples=2**14, **tasks[o][\"params\"]))\n",
    "    tasks[o][\"datasets\"].append(tasks[o][\"composer\"](n_samples=2**10, **tasks[o][\"params\"]))\n",
    "    tasks[o][\"datasets\"][-1].build_valid_test()\n",
    "    tasks[o][\"datasets\"].append(tasks[o][\"composer\"](n_samples=2**10, **tasks[o][\"params\"]))\n",
    "    tasks[o][\"datasets\"][-1].build_valid_test()\n",
    "    tasks[o][\"dataloaders\"] = build_loaders(tasks[o][\"datasets\"], batch_size=train_params[\"batch_size\"], num_workers=num_workers, pin_memory=pin_memory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a blank model\n",
    "model = AttentionModel(**model_params)\n",
    "conductor = AttentionTrain(model, None, None, tasks, logger, results_folder)\n",
    "\n",
    "# load states into the model\n",
    "model_dir = os.path.join(start_folder, \"model\" + \".pth\")\n",
    "assert os.path.exists(model_dir), \"Could not find the model.pth in the given dir!\"\n",
    "model.load_state_dict(torch.load(model_dir, map_location=DeVice))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluating...\n",
    "conductor.eval(DeVice, \"test\", False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decision accuracy\n",
    "roelfsema_ = Roelfsema(model, tasks, logger)\n",
    "roelfsema_.test_accuracy_curve(DeVice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting...\n",
    "plot_all(10, model, tasks, results_folder, \"_test\", DeVice, logger, False, \"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Curve Tracing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "tasks[\"CurveTracing\"][\"datasets\"][-1].training = False\n",
    "this_dl = DataLoader(tasks[\"CurveTracing\"][\"datasets\"][-1], batch_size=batch_size, shuffle=False)\n",
    "\n",
    "target_composites, distractor_composites, masks, rec_fields, components = next(iter(this_dl))\n",
    "target_composites = target_composites.to(DeVice)\n",
    "distractor_composites = distractor_composites.to(DeVice)\n",
    "masks = masks.to(DeVice)\n",
    "rec_fields = rec_fields.to(DeVice)\n",
    "components = components.to(DeVice)\n",
    "both_composites = (components[:, 1:2] + components[:, 4:5]).clamp(0.0, 1.0)\n",
    "\n",
    "print(target_composites.shape, distractor_composites.shape, masks.shape, rec_fields.shape, components.shape, both_composites.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "fix_attend_saccade = tasks[\"CurveTracing\"][\"params\"][\"fix_attend_saccade\"]\n",
    "n_iter = sum(fix_attend_saccade)\n",
    "n_layers = model.n_convs\n",
    "n_fix, n_att, n_sac = fix_attend_saccade\n",
    "n_fix_att = n_fix + n_att\n",
    "n_layers = model.n_convs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the receptive field\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # list_ind, base_act = get_rec_field_act(model, rec_fields)\n",
    "    \n",
    "    targets_ = get_activity(model, target_composites)\n",
    "    distractors_ = get_activity(model, distractor_composites)\n",
    "    tar_cue_ = get_activity(model, components[:, 0:1])\n",
    "    dis_cue_ = get_activity(model, components[:, 3:4])\n",
    "    both_ = get_activity(model, both_composites)[0]\n",
    "    \n",
    "    model.initiate_forward(batch_size=rec_fields.size(0))\n",
    "    *_, receptive_ = model.for_forward(rec_fields)\n",
    "    \n",
    "    tmasks_, *_ = model(target_composites)\n",
    "    dmasks_, *_ = model(distractor_composites)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot doubles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "how_many = 16\n",
    "for i in range(how_many):\n",
    "    plt.figure(figsize=(n_iter*3, 4*3))\n",
    "    for j in range(n_iter):\n",
    "        plt.subplot(4, n_iter, j + 1)\n",
    "        plt.imshow(target_composites[i, j, 0].cpu(), cmap=\"gray\")\n",
    "        plt.axis(\"off\")\n",
    "        plt.subplot(4, n_iter, n_iter + j + 1)\n",
    "        plt.imshow(tmasks_[i, j, 0].cpu(), cmap=\"plasma\")\n",
    "        plt.axis(\"off\")\n",
    "        plt.subplot(4, n_iter, 2*n_iter + j + 1)\n",
    "        plt.imshow(distractor_composites[i, j, 0].cpu(), cmap=\"gray\")\n",
    "        plt.axis(\"off\")\n",
    "        plt.subplot(4, n_iter, 3*n_iter + j + 1)\n",
    "        plt.imshow(dmasks_[i, j, 0].cpu(), cmap=\"plasma\")\n",
    "        plt.axis(\"off\")\n",
    "    plt.savefig(f\"{results_folder}/plot_bunch_{i}.svg\", format=\"svg\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Percentile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = torch.linspace(0, 0.99, 100)\n",
    "for i in range(n_iter):\n",
    "    for j in range(n_layers):\n",
    "        tar_q = torch.quantile(targets_[i][j][receptive_[j] > 0.0].ravel(), q.to(DeVice))\n",
    "        dis_q = torch.quantile(distractors_[i][j][receptive_[j] > 0.0].ravel(), q.to(DeVice))\n",
    "        mean_tar_q = torch.quantile(targets_[i][j][receptive_[j] > 0.0].ravel(), 0.5).cpu()\n",
    "        mean_dis_q = torch.quantile(distractors_[i][j][receptive_[j] > 0.0].ravel(), 0.5).cpu()\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        plt.title(f\"Layer {j}, Iteration {i}, {mean_tar_q:.2f}, {mean_dis_q:.2f}\")\n",
    "        plt.plot(tar_q.cpu(), 100.0*q, c=\"r\")\n",
    "        plt.plot(dis_q.cpu(), 100.0*q, c=\"b\")\n",
    "        plt.arrow(mean_tar_q, 50, 0.0, -45, color='r', head_width=0.05, head_length=5, alpha=1.0, width=0.01)\n",
    "        plt.arrow(mean_dis_q, 50, 0.0, -45, color='b', head_width=0.05, head_length=5, alpha=1.0, width=0.01)\n",
    "        plt.ylim(0, 100)\n",
    "        plt.xlim(0, max(tar_q.max().cpu().item(), mean_dis_q.max().cpu().item()))\n",
    "        plt.savefig(os.path.join(results_folder, f\"Percentile_layer_{j}_iter_{i}.svg\"), format=\"svg\")\n",
    "        plt.close()\n",
    "        # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = torch.linspace(0, 1.0, 51)\n",
    "tar_m_ = [[] for _ in range(n_layers)]\n",
    "dis_m_ = [[] for _ in range(n_layers)]\n",
    "for j in range(n_layers):\n",
    "    for i in range(n_fix, n_fix_att):\n",
    "        if i == n_fix:\n",
    "            tar_m_[j] = targets_[i][j][receptive_[j] > 0.0].ravel()\n",
    "            dis_m_[j] = distractors_[i][j][receptive_[j] > 0.0].ravel()\n",
    "        else:\n",
    "            tar_m_[j] = torch.cat((tar_m_[j], targets_[i][j][receptive_[j] > 0.0].ravel()), dim=0)\n",
    "            dis_m_[j] = torch.cat((dis_m_[j], distractors_[i][j][receptive_[j] > 0.0].ravel()), dim=0)\n",
    "\n",
    "for j in range(n_layers):\n",
    "    tar_q = torch.quantile(tar_m_[j], q.to(DeVice))\n",
    "    dis_q = torch.quantile(dis_m_[j], q.to(DeVice))\n",
    "    mean_tar_q = torch.quantile(tar_m_[j], 0.5).cpu()\n",
    "    mean_dis_q = torch.quantile(dis_m_[j], 0.5).cpu()\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.title(f\"Layer {j}, Iteration {n_fix}-{n_fix_att}, {mean_tar_q:.2f}, {mean_dis_q:.2f}\")\n",
    "    plt.plot(tar_q.cpu(), 100.0*q, c=\"r\")\n",
    "    plt.plot(dis_q.cpu(), 100.0*q, c=\"b\")\n",
    "    plt.arrow(mean_tar_q, 50, 0.0, -45, color='r', head_width=0.05, head_length=5, alpha=1.0, width=0.01)\n",
    "    plt.arrow(mean_dis_q, 50, 0.0, -45, color='b', head_width=0.05, head_length=5, alpha=1.0, width=0.01)\n",
    "    plt.ylim(0, 100)\n",
    "    plt.xlim(-max(tar_q.max().cpu().item(), mean_dis_q.max().cpu().item())/5, max(tar_q.max().cpu().item(), mean_dis_q.max().cpu().item()))\n",
    "    plt.savefig(os.path.join(results_folder, f\"Percentile_layer_{j}.svg\"), format=\"svg\")\n",
    "    plt.close()\n",
    "    # plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "curve_tar_act = [[] for _ in range(model.n_convs)]\n",
    "curve_dis_act = [[] for _ in range(model.n_convs)]\n",
    "for j in range(n_layers):\n",
    "    for i in range(n_iter):\n",
    "        curve_tar_act[j].append(targets_[i][j][receptive_[j] > 0.0].mean().clone())\n",
    "        curve_dis_act[j].append(distractors_[i][j][receptive_[j] > 0.0].mean().clone())\n",
    "curve_tar_act = torch.tensor(curve_tar_act)\n",
    "curve_dis_act = torch.tensor(curve_dis_act)\n",
    "plot_curves(n_layers, curve_tar_act, curve_dis_act, results_folder, \"Curve_layer\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "curve_tar_act = [[] for _ in range(model.n_convs)]\n",
    "curve_dis_act = [[] for _ in range(model.n_convs)]\n",
    "for j in range(n_layers):\n",
    "    for i in range(n_iter):\n",
    "        curve_tar_act[j].append((targets_[i][j] - both_[j])[receptive_[j] > 0.0].mean().clone())\n",
    "        curve_dis_act[j].append((distractors_[i][j] - both_[j])[receptive_[j] > 0.0].mean().clone())\n",
    "curve_tar_act = torch.tensor(curve_tar_act)\n",
    "curve_dis_act = torch.tensor(curve_dis_act)\n",
    "plot_curves(n_layers, curve_tar_act, curve_dis_act, results_folder, \"CurveDeBoth_layer\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "curve_tar_act = [[] for _ in range(model.n_convs)]\n",
    "curve_dis_act = [[] for _ in range(model.n_convs)]\n",
    "for j in range(n_layers):\n",
    "    for i in range(n_iter):\n",
    "        curve_tar_act[j].append((targets_[i][j] - both_[j])[receptive_[j] > 0.0].mean().clone())\n",
    "        curve_dis_act[j].append((distractors_[i][j] - both_[j])[receptive_[j] > 0.0].mean().clone())\n",
    "curve_tar_act = torch.tensor(curve_tar_act)\n",
    "curve_dis_act = torch.tensor(curve_dis_act)\n",
    "curve_tar_act = curve_tar_act - curve_tar_act[:, :2].mean(dim=1, keepdim=True)\n",
    "curve_dis_act = curve_dis_act - curve_dis_act[:, :2].mean(dim=1, keepdim=True)\n",
    "plot_curves(n_layers, curve_tar_act, curve_dis_act, results_folder, \"CurveDeBothDe_layer\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "curve_tar_act = [[] for _ in range(model.n_convs)]\n",
    "curve_dis_act = [[] for _ in range(model.n_convs)]\n",
    "for j in range(n_layers):\n",
    "    for i in range(n_iter):\n",
    "        curve_tar_act[j].append((targets_[i][j] - both_[j])[receptive_[j] > 0.0].mean().clone())\n",
    "        curve_dis_act[j].append((distractors_[i][j] - both_[j])[receptive_[j] > 0.0].mean().clone())\n",
    "curve_tar_act = torch.tensor(curve_tar_act)\n",
    "curve_dis_act = torch.tensor(curve_dis_act)\n",
    "curve_tar_act = curve_tar_act - curve_tar_act[:, :2].mean(dim=1, keepdim=True)\n",
    "curve_dis_act = curve_dis_act - curve_dis_act[:, :2].mean(dim=1, keepdim=True)\n",
    "curve_tar_act = curve_tar_act / curve_tar_act.max(dim=1, keepdim=True).values\n",
    "curve_dis_act = curve_dis_act / curve_tar_act.max(dim=1, keepdim=True).values\n",
    "plot_curves(n_layers, curve_tar_act, curve_dis_act, results_folder, \"CurveDeBothDeNorm_layer\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "modulation = [[] for _ in range(n_iter)]\n",
    "for i in range(n_iter):\n",
    "    for j in range(n_layers):\n",
    "        tr = targets_[i][j][receptive_[j] > 0.0]\n",
    "        dr = distractors_[i][j][receptive_[j] > 0.0]\n",
    "        tdr = ((tr - dr).abs() > 0.0)  # & (tr > 0.0) & (dr > 0.0)\n",
    "        mi = modulation_index(tr[tdr], dr[tdr])\n",
    "        mi = torch.nan_to_num(mi, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        mi = mi[(mi.abs() > 1e-6) & (mi.abs() < 1.0)]\n",
    "        modulation[i].append(mi)\n",
    "\n",
    "for i in range(n_fix, n_fix_att):\n",
    "    for j in range(n_layers):\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        median = modulation[i][j].median().cpu().item()\n",
    "        plt.hist(modulation[i][j].cpu(), bins=20, range=(-1, 1))\n",
    "        ymax = plt.gca().get_ylim()[-1]\n",
    "        plt.arrow(median, ymax, 0.0, -ymax/20, color='r', head_width=0.05, head_length=ymax/40, alpha=1.0, width=0.01)\n",
    "        plt.title(f\"Layer {j} Iter {i} Median: {median:.2f}\")\n",
    "        plt.xlim(-1, 1)\n",
    "        plt.xlabel(\"Modulation Index\")\n",
    "        plt.ylabel(\"Number of Neurons\")\n",
    "        plt.savefig(os.path.join(results_folder, f\"Modulation_layer_{j}_iter_{i}.svg\"), format=\"svg\")\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tar_m_ = [[] for _ in range(n_layers)]\n",
    "dis_m_ = [[] for _ in range(n_layers)]\n",
    "for j in range(n_layers):\n",
    "    temp_tar = []\n",
    "    temp_dis = []\n",
    "    for i in range(n_fix, n_fix_att):\n",
    "        temp_tar.append(targets_[i][j][receptive_[j] > 0.0].unsqueeze(0))\n",
    "        temp_dis.append(distractors_[i][j][receptive_[j] > 0.0].unsqueeze(0))\n",
    "    tar_m_[j] = torch.cat(temp_tar, dim=0)\n",
    "    dis_m_[j] = torch.cat(temp_dis, dim=0)\n",
    "\n",
    "    e = 1e-3\n",
    "    a = (tar_m_[j] - dis_m_[j]).abs()\n",
    "    b = (tar_m_[j]).prod(dim=0, keepdim=True)\n",
    "    c = (dis_m_[j]).prod(dim=0, keepdim=True)\n",
    "    d = (tar_m_[j] > e) & (dis_m_[j] > e) & (a > e) & (b > e) & (c > e) & (a < 1.0) & (a.prod(dim=0, keepdim=True) > e)\n",
    "    tar_m_[j] = tar_m_[j] * d\n",
    "    dis_m_[j] = dis_m_[j] * d\n",
    "    tar_m_[j] = tar_m_[j][tar_m_[j] > 0.0]\n",
    "    dis_m_[j] = dis_m_[j][dis_m_[j] > 0.0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "modulation = []\n",
    "for j in range(n_layers):\n",
    "    mi = modulation_index(tar_m_[j], dis_m_[j])\n",
    "    mi = torch.nan_to_num(mi, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    modulation.append(mi[(mi.abs() > 1e-3) & (mi.abs() < 1.0)])\n",
    "for j in range(n_layers):\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    median = modulation[j].median().cpu().item()\n",
    "    plt.hist(modulation[j].cpu(), bins=20, range=(-1, 1))\n",
    "    ymax = plt.gca().get_ylim()[-1]\n",
    "    plt.arrow(median, ymax, 0.0, -ymax/20, color='r', head_width=0.05, head_length=ymax/40, alpha=1.0, width=0.01)\n",
    "    plt.title(f\"Layer {j} Iter {n_fix}-{n_fix_att} Median: {median:.2f}\")\n",
    "    plt.xlim(-1, 1)\n",
    "    plt.xlabel(\"Modulation Index\")\n",
    "    plt.ylabel(\"Number of Neurons\")\n",
    "    plt.savefig(os.path.join(results_folder, f\"Modulation_layer_{j}_iter_{n_fix}-{n_fix_att}.svg\"), format=\"svg\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "modulation = [[] for _ in range(n_iter)]\n",
    "for i in range(n_iter):\n",
    "    for j in range(n_layers):\n",
    "        tr = (targets_[i][j] - both_[j])[receptive_[j] > 0.0]\n",
    "        dr = (distractors_[i][j] - both_[j])[receptive_[j] > 0.0]\n",
    "        tdr = ((tr - dr).abs() > 0.0)  # & (tr > 0.0) & (dr > 0.0)\n",
    "        mi = modulation_index(tr[tdr], dr[tdr])\n",
    "        mi = torch.nan_to_num(mi, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        mi = mi[(mi.abs() > 1e-6) & (mi.abs() < 1.0)]\n",
    "        modulation[i].append(mi)\n",
    "\n",
    "for i in range(n_fix, n_fix_att):\n",
    "    for j in range(n_layers):\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        median = modulation[i][j].median().cpu().item()\n",
    "        plt.hist(modulation[i][j].cpu(), bins=20, range=(-1, 1))\n",
    "        ymax = plt.gca().get_ylim()[-1]\n",
    "        plt.arrow(median, ymax, 0.0, -ymax/20, color='r', head_width=0.05, head_length=ymax/40, alpha=1.0, width=0.01)\n",
    "        plt.title(f\"Layer {j} Iter {i} Median: {median:.2f}\")\n",
    "        plt.xlim(-1, 1)\n",
    "        plt.xlabel(\"Modulation Index\")\n",
    "        plt.ylabel(\"Number of Neurons\")\n",
    "        plt.savefig(os.path.join(results_folder, f\"ModulationDe_layer_{j}_iter_{i}.svg\"), format=\"svg\")\n",
    "        plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "tar_m_ = [[] for _ in range(n_layers)]\n",
    "dis_m_ = [[] for _ in range(n_layers)]\n",
    "for j in range(n_layers):\n",
    "    for i in range(n_fix, n_fix_att):\n",
    "        tr = (targets_[i][j] - both_[j])[receptive_[j] > 0.0]\n",
    "        dr = (distractors_[i][j] - both_[j])[receptive_[j] > 0.0]\n",
    "        if i == n_fix:\n",
    "            tar_m_[j] = tr[((tr - dr).abs() > 0.0)]\n",
    "            dis_m_[j] = dr[((tr - dr).abs() > 0.0)]\n",
    "        else:\n",
    "            tar_m_[j] = torch.cat((tar_m_[j], tr[((tr - dr).abs() > 0.0)]), dim=0)\n",
    "            dis_m_[j] = torch.cat((dis_m_[j], dr[((tr - dr).abs() > 0.0)]), dim=0)\n",
    "\n",
    "modulation = []\n",
    "for j in range(n_layers):\n",
    "    mi = modulation_index(tar_m_[j], dis_m_[j])\n",
    "    mi = torch.nan_to_num(mi, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    modulation.append(mi[(mi.abs() > 1e-6) & (mi.abs() < 1.0)])\n",
    "\n",
    "for j in range(n_layers):\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    median = modulation[j].median().cpu().item()\n",
    "    plt.hist(modulation[j].cpu(), bins=20, range=(-1, 1))\n",
    "    ymax = plt.gca().get_ylim()[-1]\n",
    "    plt.arrow(median, ymax, 0.0, -ymax/20, color='r', head_width=0.05, head_length=ymax/40, alpha=1.0, width=0.01)\n",
    "    plt.title(f\"Layer {j} Iter {n_fix}-{n_fix_att} Median: {median:.2f}\")\n",
    "    plt.xlim(-1, 1)\n",
    "    plt.xlabel(\"Modulation Index\")\n",
    "    plt.ylabel(\"Number of Neurons\")\n",
    "    plt.savefig(os.path.join(results_folder, f\"ModulationDe_layer_{j}_iter_{n_fix}-{n_fix_att}.svg\"), format=\"svg\")\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Invariant Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_res = 180\n",
    "stimulie, tar_cues, dis_cues, tar_stim = make_stimuli(128, 128, theta_res, bar=True, ds=this_dl.dataset)\n",
    "stimulie, tar_cues, dis_cues = (x.unsqueeze(1) for x in (stimulie, tar_cues, dis_cues))\n",
    "tccsss = torch.cat([*(tar_cues for _ in range(n_fix)), *(stimulie for _ in range(n_att))], dim=1)\n",
    "dccsss = torch.cat([*(dis_cues for _ in range(n_fix)), *(stimulie for _ in range(n_att))], dim=1)\n",
    "model.to(DeVice)\n",
    "stimulie = stimulie.to(DeVice)\n",
    "tar_cues = tar_cues.to(DeVice)\n",
    "dis_cues = dis_cues.to(DeVice)\n",
    "tar_stim = tar_stim.to(DeVice)\n",
    "tccsss = tccsss.to(DeVice)\n",
    "dccsss = dccsss.to(DeVice)\n",
    "both_stim = tccsss[:, -1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    tmasks_, *_ = model(tccsss)\n",
    "    dmasks_, *_ = model(dccsss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bi = 45\n",
    "plt.figure(figsize=(12, 3))\n",
    "plt.subplot(141)\n",
    "plt.imshow(stimulie[bi][0][0].cpu())\n",
    "plt.axis(\"off\")\n",
    "plt.subplot(142)\n",
    "plt.imshow(tar_cues[bi][0][0].cpu())\n",
    "plt.axis(\"off\")\n",
    "plt.subplot(143)\n",
    "plt.imshow(dis_cues[bi][0][0].cpu())\n",
    "plt.axis(\"off\")\n",
    "plt.subplot(144)\n",
    "plt.imshow(tar_stim[bi][0].cpu())\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bi = 45\n",
    "plt.figure(figsize=(n_fix_att*2, 2*2))\n",
    "for i in range(n_fix_att):\n",
    "    plt.subplot(2, n_fix_att, i+1)\n",
    "    plt.imshow(tmasks_[bi, i, 0].detach().cpu(), vmax=1.0, vmin=-1.0, cmap=\"plasma\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.subplot(2, n_fix_att, i+1+n_fix_att)\n",
    "    plt.imshow(dmasks_[bi, i, 0].detach().cpu(), vmax=1.0, vmin=-1.0, cmap=\"plasma\")\n",
    "    plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list_ind, base_act = get_rec_field_act(model, tar_stim, e=0)\n",
    "targets_ = get_activity(model, tccsss)  # iter x layer x batch x channel x h x w\n",
    "distractors_ = get_activity(model, dccsss)\n",
    "both_ = get_activity(model, both_stim)\n",
    "single_ = get_activity(model, tar_stim)[0]\n",
    "cue_tar = get_activity(model, tccsss[:, :n_fix])\n",
    "cue_dis = get_activity(model, dccsss[:, :n_fix])\n",
    "\n",
    "rec_fields = (tar_stim - tccsss[:, 0]).clamp(0.0, 1.0)\n",
    "model.initiate_forward(batch_size=rec_fields.size(0))\n",
    "*_, receptive_ = model.for_forward(rec_fields)\n",
    "\n",
    "\n",
    "cue_tar_tns = [[] for _ in range(n_layers)]\n",
    "cue_dis_tns = [[] for _ in range(n_layers)]\n",
    "for layer_ in range(n_layers):\n",
    "    for iter_ in range(n_fix):\n",
    "        if iter_ == 0:\n",
    "            cue_tar_tns[layer_] = cue_tar[0][layer_].unsqueeze(0)\n",
    "            cue_dis_tns[layer_] = cue_dis[0][layer_].unsqueeze(0)\n",
    "        else:\n",
    "            cue_tar_tns[layer_] = torch.cat([cue_tar_tns[layer_], cue_tar[iter_][layer_].unsqueeze(0)], dim=0)\n",
    "            cue_dis_tns[layer_] = torch.cat([cue_dis_tns[layer_], cue_dis[iter_][layer_].unsqueeze(0)], dim=0)\n",
    "    cue_tar_tns[layer_] = cue_tar_tns[layer_].mean(dim=0)\n",
    "    cue_dis_tns[layer_] = cue_dis_tns[layer_].mean(dim=0)\n",
    "\n",
    "\n",
    "fit_gaussian = FitBellCurve()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = 1e-3\n",
    "iter_ = 4\n",
    "layer_ = 6\n",
    "for iter_ in range(n_fix, n_fix_att):\n",
    "    for layer_ in range(model.n_convs):\n",
    "        tar_i_l = targets_[iter_][layer_]  # - cue_tar_tns[layer_]\n",
    "        dis_i_l = distractors_[iter_][layer_]  # - cue_tar_tns[layer_]\n",
    "        sin_i_l = single_[layer_]\n",
    "        both_i_l = both_[0][layer_]\n",
    "        base_i_l = receptive_[layer_]\n",
    "\n",
    "        s_tar_i_l = tar_i_l.diff(dim=0).abs().sum(dim=0, keepdim=True)\n",
    "        s_dis_i_l = dis_i_l.diff(dim=0).abs().sum(dim=0, keepdim=True)\n",
    "        s_sin_i_l = sin_i_l.diff(dim=0).abs().sum(dim=0, keepdim=True)\n",
    "        s_both_i_l = both_i_l.diff(dim=0).abs().sum(dim=0, keepdim=True)\n",
    "        s_base_i_l = base_i_l.diff(dim=0).abs().sum(dim=0, keepdim=True)\n",
    "\n",
    "        s_tar_dis_i_l = (tar_i_l - dis_i_l).abs().sum(dim=0, keepdim=True)\n",
    "        # s_i_l = (s_tar_i_l > e) & (s_dis_i_l > e) & (s_sin_i_l > e) & (s_tar_dis_i_l > e)\n",
    "        s_i_l = (s_tar_i_l > e) & (s_dis_i_l > e) & (s_base_i_l > e) & (s_tar_dis_i_l > e)\n",
    "\n",
    "        p_s_i_l = s_i_l.permute(1, 2, 3, 0)\n",
    "\n",
    "        p_tar_i_l = tar_i_l.permute(1, 2, 3, 0)[p_s_i_l[..., 0]].detach().cpu() # .numpy()\n",
    "        p_dis_i_l = dis_i_l.permute(1, 2, 3, 0)[p_s_i_l[..., 0]].detach().cpu() # .numpy()\n",
    "        p_sin_i_l = sin_i_l.permute(1, 2, 3, 0)[p_s_i_l[..., 0]].detach().cpu() # .numpy()\n",
    "        p_both_i_l = both_i_l.permute(1, 2, 3, 0)[p_s_i_l[..., 0]].detach().cpu() # .numpy()\n",
    "        p_base_i_l = base_i_l.permute(1, 2, 3, 0)[p_s_i_l[..., 0]].detach().cpu() # .numpy()\n",
    "\n",
    "        p_tar_i_l = torch.tensor(gaussian_filter1d(p_tar_i_l, 2, axis=-1))\n",
    "        p_dis_i_l = torch.tensor(gaussian_filter1d(p_dis_i_l, 2, axis=-1))\n",
    "        p_sin_i_l = torch.tensor(gaussian_filter1d(p_sin_i_l, 2, axis=-1))\n",
    "        p_both_i_l = torch.tensor(gaussian_filter1d(p_both_i_l, 2, axis=-1))\n",
    "        p_base_i_l = torch.tensor(gaussian_filter1d(p_base_i_l, 2, axis=-1))\n",
    "\n",
    "        good_tar_ = []\n",
    "        good_dis_ = []\n",
    "        good_ones = []\n",
    "        for i in range(p_tar_i_l.size(0)):\n",
    "            (ta, tb, tc, td), te = fit_gaussian(p_tar_i_l[i])\n",
    "            (da, db, dc, dd), de = fit_gaussian(p_dis_i_l[i])\n",
    "            if te < 0.01 and de < 0.01:\n",
    "                good_ones.append(i)\n",
    "                good_tar_.append([ta, tb, tc, td])\n",
    "                good_dis_.append([da, db, dc, dd])\n",
    "                if i%100 == 0:\n",
    "                    polar_plot(p_tar_i_l[i], p_dis_i_l[i], 180, results_folder, f\"_good_L{layer_}_i{iter_}_N{i}\")\n",
    "            else:\n",
    "                if i%100 == 0:\n",
    "                    polar_plot(p_tar_i_l[i], p_dis_i_l[i], 180, results_folder, f\"_bad_L{layer_}_i{iter_}_N{i}\")\n",
    "        logger.info(f\"iter: {iter_}, layer: {layer_}, all: {s_i_l.numel()}, good: {s_i_l.sum().item()}, fit: {len(good_ones)}\")\n",
    "\n",
    "        good_tar_ = torch.tensor(good_tar_)\n",
    "        good_dis_ = torch.tensor(good_dis_)\n",
    "        for i, n in enumerate((\"Amp\", \"Asymp\", \"Width\", \"Pref\")):\n",
    "            i = modulation_index(good_tar_[:, i], good_dis_[:, i])\n",
    "            plt.figure(figsize=(4, 3))\n",
    "            plt.title(f\"Layer: {layer_}, Iter: {iter_} Plot: {n} Median: {i.median().item():.2f}\")\n",
    "            plt.hist(i, bins=min(20, max(len(i)//10, 20)), range=(-1, 1))\n",
    "            ymax = plt.gca().get_ylim()[-1]\n",
    "            plt.arrow(i.median().item(), ymax, 0.0, -ymax/20, color='r', head_width=0.05, head_length=ymax/40, alpha=1.0, width=0.01)\n",
    "\n",
    "            plt.savefig(os.path.join(results_folder, f\"AAA_TuningNorm_{n}_layer_{layer_}_iter_{iter_}.svg\"), format=\"svg\")\n",
    "            plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = 1e-3\n",
    "iter_ = 4\n",
    "layer_ = 6\n",
    "tar_i_l = targets_[iter_][layer_]  # - cue_tar_tns[layer_]\n",
    "dis_i_l = distractors_[iter_][layer_]  # - cue_tar_tns[layer_]\n",
    "sin_i_l = single_[layer_]\n",
    "both_i_l = both_[0][layer_]\n",
    "base_i_l = receptive_[layer_]\n",
    "\n",
    "s_tar_i_l = tar_i_l.diff(dim=0).abs().sum(dim=0, keepdim=True)\n",
    "s_dis_i_l = dis_i_l.diff(dim=0).abs().sum(dim=0, keepdim=True)\n",
    "s_sin_i_l = sin_i_l.diff(dim=0).abs().sum(dim=0, keepdim=True)\n",
    "s_both_i_l = both_i_l.diff(dim=0).abs().sum(dim=0, keepdim=True)\n",
    "s_base_i_l = base_i_l.diff(dim=0).abs().sum(dim=0, keepdim=True)\n",
    "\n",
    "s_tar_dis_i_l = (tar_i_l - dis_i_l).abs().sum(dim=0, keepdim=True)\n",
    "# s_i_l = (s_tar_i_l > e) & (s_dis_i_l > e) & (s_sin_i_l > e) & (s_tar_dis_i_l > e)\n",
    "s_i_l = (s_tar_i_l > e) & (s_dis_i_l > e) & (s_base_i_l > e) & (s_tar_dis_i_l > e)\n",
    "\n",
    "p_s_i_l = s_i_l.permute(1, 2, 3, 0)\n",
    "\n",
    "p_tar_i_l = tar_i_l.permute(1, 2, 3, 0)[p_s_i_l[..., 0]].detach().cpu() # .numpy()\n",
    "p_dis_i_l = dis_i_l.permute(1, 2, 3, 0)[p_s_i_l[..., 0]].detach().cpu() # .numpy()\n",
    "p_sin_i_l = sin_i_l.permute(1, 2, 3, 0)[p_s_i_l[..., 0]].detach().cpu() # .numpy()\n",
    "p_both_i_l = both_i_l.permute(1, 2, 3, 0)[p_s_i_l[..., 0]].detach().cpu() # .numpy()\n",
    "p_base_i_l = base_i_l.permute(1, 2, 3, 0)[p_s_i_l[..., 0]].detach().cpu() # .numpy()\n",
    "\n",
    "p_tar_i_l = torch.tensor(gaussian_filter1d(p_tar_i_l, 2, axis=-1))\n",
    "p_dis_i_l = torch.tensor(gaussian_filter1d(p_dis_i_l, 2, axis=-1))\n",
    "p_sin_i_l = torch.tensor(gaussian_filter1d(p_sin_i_l, 2, axis=-1))\n",
    "p_both_i_l = torch.tensor(gaussian_filter1d(p_both_i_l, 2, axis=-1))\n",
    "p_base_i_l = torch.tensor(gaussian_filter1d(p_base_i_l, 2, axis=-1))\n",
    "\n",
    "good_tar_ = []\n",
    "good_dis_ = []\n",
    "good_ones = []\n",
    "for i in range(p_tar_i_l.size(0)):\n",
    "    (ta, tb, tc, td), te = fit_gaussian(p_tar_i_l[i])\n",
    "    (da, db, dc, dd), de = fit_gaussian(p_dis_i_l[i])\n",
    "    if te < 0.01 and de < 0.01:\n",
    "        good_ones.append(i)\n",
    "        good_tar_.append([ta, tb, tc, td])\n",
    "        good_dis_.append([da, db, dc, dd])\n",
    "        if i%50 == 0:\n",
    "            polar_plot(p_tar_i_l[i], p_dis_i_l[i], 180, results_folder, f\"_c_good_L{layer_}_i{iter_}_N{i}\")\n",
    "    else:\n",
    "        if i%50 == 0:\n",
    "            polar_plot(p_tar_i_l[i], p_dis_i_l[i], 180, results_folder, f\"_c_bad_L{layer_}_i{iter_}_N{i}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
