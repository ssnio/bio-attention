{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # header\n",
    "import sys\n",
    "sys.path.append(r\"../\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # built-in modules\n",
    "import os\n",
    "import argparse\n",
    "from pprint import pformat\n",
    "from collections import OrderedDict\n",
    "import random\n",
    "# # Torch modules\n",
    "import torch\n",
    "from torchvision import transforms, datasets\n",
    "# # internal imports\n",
    "from prelude import startup_folders, get_device, load_dicts\n",
    "from src.composer import CelebACrop, CelebGender\n",
    "from src.model import AttentionModel\n",
    "from src.utils import plot_all\n",
    "from src.utils import build_loaders\n",
    "from src.conductor import AttentionTrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../pretrained/celeba/1728906947 was created!\n"
     ]
    }
   ],
   "source": [
    "start_folder = r\"../pretrained/celeba\"\n",
    "results_folder, logger = startup_folders(start_folder, name=f\"exp_celeb\")\n",
    "data_path = r\"../data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device set to mps\n",
      "model_params: {'in_dims': [3, 128, 128], 'n_classes': 2, 'out_dim': 2, 'normalize': True, 'softness': 0.5, 'channels': [3, 4, 8, 8, 16, 16, 32, 32], 'residuals': False, 'kernels': 3, 'strides': 1, 'paddings': 1, 'conv_bias': True, 'conv_norms': None, 'conv_dropouts': 0.0, 'conv_funs': ReLU(), 'deconv_funs': Tanh(), 'deconv_norms': None, 'pools': 2, 'rnn_dims': [32, 8], 'rnn_bias': True, 'rnn_dropouts': 0.0, 'rnn_funs': ReLU(), 'n_tasks': 1, 'task_weight': False, 'task_bias': False, 'task_funs': None, 'rnn_to_fc': True, 'trans_fun': Identity(), 'norm_mean': [0.5, 0.5, 0.5], 'norm_std': [1.0, 1.0, 1.0]}\n",
      "tasks: {'Celeb': {'composer': 'CelebACrop', 'key': 0, 'params': {'n_iter': 2, 'hair_dir': None, 'in_dims': [3, 128, 128], 'padding': 0, 'noise': 0.25, 'which': 0}, 'datasets': ['CelebACrop', 'CelebACrop', 'CelebACrop'], 'dataloaders': [None, None, None], 'loss_w': [0.0, 0.0, 1.0], 'loss_s': [0, None], 'has_prompt': False}}\n",
      "train_params: {'n_epochs': 32, 'batch_size': 128, 'lr': 0.0001, 'l2': 0.0005, 'exase': 'Y02', 'dir': './results', 'milestones': [16], 'gamma': 0.1, 'max_grad_norm': 1.0}\n"
     ]
    }
   ],
   "source": [
    "model_params = load_dicts(start_folder, \"model_params\")\n",
    "tsk = load_dicts(start_folder, \"tasks\")\n",
    "train_params = load_dicts(start_folder, \"train_params\")\n",
    "n_iter = tsk[\"Celeb\"]['params']['n_iter']\n",
    "DeVice, num_workers, pin_memory = get_device()\n",
    "print(f\"model_params: {model_params}\")\n",
    "print(f\"tasks: {tsk}\")\n",
    "print(f\"train_params: {train_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using \"which\" parameter in the class to select the hair color and gender \n",
    "tasks = OrderedDict({})\n",
    "tasks[\"Celeb_all\"] = {\n",
    "    \"composer\": CelebACrop,  # composer (torch Dataset)\n",
    "    \"key\": 0,  # key for the task\n",
    "    \"params\": {\"n_iter\": n_iter, \"hair_dir\": data_path, \"in_dims\": model_params[\"in_dims\"], \"padding\": 0, \"noise\": 0.25, \"which\": 0},\n",
    "    \"datasets\": [],\n",
    "    \"dataloaders\": [],\n",
    "    \"loss_w\": (0.0, 0.0, 1.0),  # Loss weights (Cross-Entropy (CE), MSE for attention, CE last label)\n",
    "    \"loss_s\": (None, None),  # Loss slices (CE, MSE for attention)\n",
    "    \"has_prompt\": False,  # has prompt or not (only used for top-down Search)\n",
    "}\n",
    "tasks[\"Celeb_fblonde\"] = {\n",
    "    \"composer\": CelebACrop,  # composer (torch Dataset)\n",
    "    \"key\": 0,  # key for the task\n",
    "    \"params\": {\"n_iter\": n_iter, \"hair_dir\": data_path, \"in_dims\": model_params[\"in_dims\"], \"padding\": 0, \"noise\": 0.25, \"which\": 1},\n",
    "    \"datasets\": [],\n",
    "    \"dataloaders\": [],\n",
    "    \"loss_w\": (0.0, 0.0, 1.0),  # Loss weights (Cross-Entropy (CE), MSE for attention, CE last label)\n",
    "    \"loss_s\": (None, None),  # Loss slices (CE, MSE for attention)\n",
    "    \"has_prompt\": False,  # has prompt or not (only used for top-down Search)\n",
    "}\n",
    "tasks[\"Celeb_fblack\"] = {\n",
    "    \"composer\": CelebACrop,  # composer (torch Dataset)\n",
    "    \"key\": 0,  # key for the task\n",
    "    \"params\": {\"n_iter\": n_iter, \"hair_dir\": data_path, \"in_dims\": model_params[\"in_dims\"], \"padding\": 0, \"noise\": 0.25, \"which\": 2},\n",
    "    \"datasets\": [],\n",
    "    \"dataloaders\": [],\n",
    "    \"loss_w\": (0.0, 0.0, 1.0),  # Loss weights (Cross-Entropy (CE), MSE for attention, CE last label)\n",
    "    \"loss_s\": (None, None),  # Loss slices (CE, MSE for attention)\n",
    "    \"has_prompt\": False,  # has prompt or not (only used for top-down Search)\n",
    "}\n",
    "tasks[\"Celeb_mblonde\"] = {\n",
    "    \"composer\": CelebACrop,  # composer (torch Dataset)\n",
    "    \"key\": 0,  # key for the task\n",
    "    \"params\": {\"n_iter\": n_iter, \"hair_dir\": data_path, \"in_dims\": model_params[\"in_dims\"], \"padding\": 0, \"noise\": 0.25, \"which\": 3},\n",
    "    \"datasets\": [],\n",
    "    \"dataloaders\": [],\n",
    "    \"loss_w\": (0.0, 0.0, 1.0),  # Loss weights (Cross-Entropy (CE), MSE for attention, CE last label)\n",
    "    \"loss_s\": (None, None),  # Loss slices (CE, MSE for attention)\n",
    "    \"has_prompt\": False,  # has prompt or not (only used for top-down Search)\n",
    "}\n",
    "tasks[\"Celeb_mblack\"] = {\n",
    "    \"composer\": CelebACrop,  # composer (torch Dataset)\n",
    "    \"key\": 0,  # key for the task\n",
    "    \"params\": {\"n_iter\": n_iter, \"hair_dir\": data_path, \"in_dims\": model_params[\"in_dims\"], \"padding\": 0, \"noise\": 0.25, \"which\": 4},\n",
    "    \"datasets\": [],\n",
    "    \"dataloaders\": [],\n",
    "    \"loss_w\": (0.0, 0.0, 1.0),  # Loss weights (Cross-Entropy (CE), MSE for attention, CE last label)\n",
    "    \"loss_s\": (None, None),  # Loss slices (CE, MSE for attention)\n",
    "    \"has_prompt\": False,  # has prompt or not (only used for top-down Search)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device set to mps\n",
      "Loading train_hair_ids.pt from file!\n",
      "Loading valid_hair_ids.pt from file!\n",
      "Loading test_hair_ids.pt from file!\n",
      "Loading train_hair_ids.pt from file!\n",
      "Loading valid_hair_ids.pt from file!\n",
      "Loading test_hair_ids.pt from file!\n",
      "Loading train_hair_ids.pt from file!\n",
      "Loading valid_hair_ids.pt from file!\n",
      "Loading test_hair_ids.pt from file!\n",
      "Loading train_hair_ids.pt from file!\n",
      "Loading valid_hair_ids.pt from file!\n",
      "Loading test_hair_ids.pt from file!\n",
      "Loading train_hair_ids.pt from file!\n",
      "Loading valid_hair_ids.pt from file!\n",
      "Loading test_hair_ids.pt from file!\n"
     ]
    }
   ],
   "source": [
    "# datasets and dataloaders\n",
    "train_ds = datasets.CelebA(root=data_path, split='train', transform=transforms.ToTensor())\n",
    "valid_ds = datasets.CelebA(root=data_path, split='valid', transform=transforms.ToTensor())\n",
    "test_ds = datasets.CelebA(root=data_path, split='test', transform=transforms.ToTensor())\n",
    "DeVice, num_workers, pin_memory = get_device()\n",
    "for o in tasks:\n",
    "    # if o == \"CelebGender\":\n",
    "    #     tasks[o][\"datasets\"].append(tasks[o][\"composer\"](train_ds, **tasks[o][\"params\"], kind=\"train\"))\n",
    "    #     tasks[o][\"datasets\"].append(tasks[o][\"composer\"](valid_ds, **tasks[o][\"params\"], kind=\"valid\"))\n",
    "    #     tasks[o][\"datasets\"].append(tasks[o][\"composer\"](test_ds, **tasks[o][\"params\"], kind=\"test\"))\n",
    "    # else:\n",
    "    tasks[o][\"datasets\"].append(tasks[o][\"composer\"](train_ds, **tasks[o][\"params\"], kind=\"train\"))\n",
    "    tasks[o][\"datasets\"].append(tasks[o][\"composer\"](valid_ds, **tasks[o][\"params\"], kind=\"valid\"))\n",
    "    tasks[o][\"datasets\"].append(tasks[o][\"composer\"](test_ds, **tasks[o][\"params\"], kind=\"test\"))\n",
    "    tasks[o][\"dataloaders\"] = build_loaders(tasks[o][\"datasets\"], batch_size=train_params[\"batch_size\"], num_workers=num_workers, pin_memory=pin_memory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a blank model\n",
    "model = AttentionModel(**model_params)\n",
    "conductor = AttentionTrain(model, None, None, tasks, logger, results_folder)\n",
    "\n",
    "# load states into the model\n",
    "model_dir = os.path.join(start_folder, \"model\" + \".pth\")\n",
    "assert os.path.exists(model_dir), \"Could not find the model.pth in the given dir!\"\n",
    "model.load_state_dict(torch.load(model_dir, map_location=DeVice))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting...\n",
    "plot_all(10, model, tasks, results_folder, \"_test\", DeVice, logger, False, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Task Celeb_all:\n",
      "    CEi Loss: 5.390714    CEe Loss: 0.163816    Pix Err: 0.000000    Att Acc: 0.000000    Cls Acc: 7526/8082\n",
      "  Task Celeb_fblonde:\n",
      "    CEi Loss: 5.113946    CEe Loss: 0.091970    Pix Err: 0.000000    Att Acc: 0.000000    Cls Acc: 2385/2480\n",
      "  Task Celeb_fblack:\n",
      "    CEi Loss: 8.126309    CEe Loss: 0.252569    Pix Err: 0.000000    Att Acc: 0.000000    Cls Acc: 2562/2875\n",
      "  Task Celeb_mblonde:\n",
      "    CEi Loss: 3.405327    CEe Loss: 0.745578    Pix Err: 0.000000    Att Acc: 0.000000    Cls Acc: 110/180\n",
      "  Task Celeb_mblack:\n",
      "    CEi Loss: 2.712632    CEe Loss: 0.092475    Pix Err: 0.000000    Att Acc: 0.000000    Cls Acc: 2469/2547\n"
     ]
    }
   ],
   "source": [
    "# evaluating...\n",
    "conductor.eval(DeVice, \"test\", False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
